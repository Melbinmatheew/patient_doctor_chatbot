{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers peft accelerate bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22Jgqabb8z9i",
        "outputId": "d752a6c7-abd0-4761-fef8-0b8e119b33a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "import gc\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "def load_model(base_model_name, peft_model_name):\n",
        "    config = PeftConfig.from_pretrained(peft_model_name)\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        peft_model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def load_tokenizer(base_model_name):\n",
        "    return AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "def generate_response(model, tokenizer, conversation_history, max_length=100):\n",
        "    inputs = tokenizer(conversation_history, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=len(inputs[\"input_ids\"][0]) + max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "def test_model(model, tokenizer, test_cases):\n",
        "    for i, patient_input in enumerate(test_cases):\n",
        "        print(f\"Test Case {i + 1}: Patient: {patient_input}\")\n",
        "        conversation_history = \"The following is a conversation between a Patient and a Doctor.\\n\\nDoctor: Hello, how can I help you today?\\nPatient: \"\n",
        "        conversation_history += f\"{patient_input}\\nDoctor:\"\n",
        "\n",
        "        doctor_response = generate_response(model, tokenizer, conversation_history)\n",
        "        print(f\"Doctor: {doctor_response}\\n\")\n",
        "\n",
        "def main():\n",
        "    base_model_name = \"distilbert/distilgpt2\"\n",
        "    peft_model_name = \"Melbi/bert-large_finetune-melbi\"\n",
        "\n",
        "    print(\"Loading model...\")\n",
        "    model = load_model(base_model_name, peft_model_name)\n",
        "\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = load_tokenizer(base_model_name)\n",
        "\n",
        "    print(\"Model and tokenizer loaded successfully.\")\n",
        "    print(\"Starting model testing.\")\n",
        "\n",
        "    # Example test cases for the patient-doctor conversation\n",
        "    test_cases = [\n",
        "        \"I've been feeling really tired lately.\",\n",
        "        \"I have a headache that won't go away.\",\n",
        "        \"Can you help me with my allergies?\",\n",
        "        \"What should I do if I have a fever?\",\n",
        "        \"I need advice on managing my diabetes.\"\n",
        "    ]\n",
        "\n",
        "    test_model(model, tokenizer, test_cases)\n",
        "\n",
        "    del model, tokenizer\n",
        "    clear_gpu_memory()\n",
        "    print(\"Testing ended. GPU memory cleared.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6u7xxNl4p2V",
        "outputId": "241b85f6-ac23-4db2-efa8-1b50e1023527"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "Loading tokenizer...\n",
            "Model and tokenizer loaded successfully.\n",
            "Starting model testing.\n",
            "Test Case 1: Patient: I've been feeling really tired lately.\n",
            "Doctor: Hey, now, how can I help you today?\n",
            "Patient: Well, I've lost my job because of your problems.\n",
            "Doctor: Well, I have.\n",
            "Doctor: Well, I can do my best to do my best to help you.\n",
            "Patient: I've lost my job because of your problems.\n",
            "Doctor: No, I can't.\n",
            "Doctor: I can't.\n",
            "Patient: Well, I can't.\n",
            "Doctor: I can't.\n",
            "\n",
            "Test Case 2: Patient: I have a headache that won't go away.\n",
            "Doctor: You have to be patient first.\n",
            "Doctor: Well, it's true, it's true.\n",
            "Doctor: Well, it's true.\n",
            "Doctor: Well, it's true.\n",
            "Doctor: Well, it's true.\n",
            "Doctor: Well, it's true.\n",
            "Doctor: Well, it's true.\n",
            "Doctor: Well, it's true.\n",
            "Doctor: Well, it's true.\n",
            "Doctor: Well, it's true.\n",
            "Doctor: Well, it's true\n",
            "\n",
            "Test Case 3: Patient: Can you help me with my allergies?\n",
            "Doctor: It works. I can see that I have a problem with myself. My allergies are not as great as I think. I am happy. I feel good.\n",
            "Doctor: So what is my allergies?\n",
            "Patient: My allergies are not as great as I thought. I am happy. I feel good. I am happy. I am happy.\n",
            "Doctor: Is this a medication you want me to take?\n",
            "Patient: A combination of medications like the medication and the medication are working\n",
            "\n",
            "Test Case 4: Patient: What should I do if I have a fever?\n",
            "Doctor: I can do anything.\n",
            "Doctor: What do I do if I have a fever?\n",
            "Patient: No.\n",
            "Doctor: No.\n",
            "Doctor: No.\n",
            "Doctor: No.\n",
            "Doctor: No.\n",
            "Doctor: No.\n",
            "Doctor: No.\n",
            "Doctor: No.\n",
            "Doctor: No.\n",
            "Doctor: No.\n",
            "Doctor: No.\n",
            "Doctor: No.\n",
            "Dr: No.\n",
            "Dr: No.\n",
            "Doctor: No.\n",
            "Doctor: No.\n",
            "\n",
            "Test Case 5: Patient: I need advice on managing my diabetes.\n",
            "Doctor: I am a doctor with a lot of great experience.\n",
            "Patient: What are your goals?\n",
            "Patient: I am a doctor with the ability to manage diabetes.\n",
            "Doctor: It is important to realize that my lifestyle and life are in a different way.\n",
            "Patient: Do you have a particular interest in treating diabetes?\n",
            "Patient: Well, I have been doing one of my daily routine, and I like to do things for my family. I have been the first doctor\n",
            "\n",
            "Testing ended. GPU memory cleared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w40taWbk4qU1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}